{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Project\n",
    "\n",
    "By Nan Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string \n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning \n",
    "\n",
    "-  EDA: \n",
    "    What we can tell from the violinplot? Should we remove these two plots from our EDA part?\n",
    "    Similarly, from word cloud, it seems all types have similar dominant words\n",
    "    Setiment Score distribution is highly skewed? Should we just remove this feature, or any other ways to normalize this feature\n",
    "\n",
    "- Featuring Engineering:\n",
    "    - Count Vectors as featuress or TF-IDF Vectors as features(N-gram Level TFIDF, set ngram=(1,2)\n",
    "    - Text, NLP based features(word count, upper case count, number count, etc)\n",
    "\n",
    "* I vs E, N vs S are highly imbalanced, which will affect the models. Do we need to apply some techiniques mentioned here?reference: https://elitedatascience.com/imbalanced-classes\n",
    "\n",
    "- Modeling:\n",
    "    - Naive Bayes Classifer(in FirstModel file)\n",
    "    \n",
    "    - KNN\n",
    "    - Baggings -Random Forest\n",
    "        - Random Search CV on RF:\n",
    "            - {'bootstrap': [True, False],\n",
    "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}    \n",
    "        - Best Parameters:\n",
    "        \n",
    "        - Results: F1-score = , ROC-AUC = \n",
    "\n",
    "\n",
    "- Performance Metrics\n",
    "    - Accuracy(removed as imbalanced classes), F1-score, ROC-AUC which one should we prioritize for our project? \n",
    "    -  From the production perspective, we can add matrics like computation cost, time complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info:\n",
    "#### Personality Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>processed_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>moment    sportscenter top ten play    pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>finding lack post alarming  sex boring positi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one           course  say know  blessing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear     enjoyed conversation day   esoteric ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>fired  another silly misconception  approachi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                     processed_posts  \n",
       "0       moment    sportscenter top ten play    pr...  \n",
       "1   finding lack post alarming  sex boring positi...  \n",
       "2   good one           course  say know  blessing...  \n",
       "3   dear     enjoyed conversation day   esoteric ...  \n",
       "4   fired  another silly misconception  approachi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save time, run this to load the clean post\n",
    "processed_post = pd.read_csv('data/mbti_preprocessed_1.csv')\n",
    "processed_post.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "processed_post.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of each user by using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=0.05, max_df=0.85, analyzer='word', ngram_range=(1, 2))\n",
    "word_tfidf = vectorizer_tfidf.fit_transform(processed_post['processed_posts'])\n",
    "word_tfidf_df = pd.DataFrame(data = word_tfidf.toarray(), columns = vectorizer_tfidf.get_feature_names())\n",
    "# CountVectorizer\n",
    "vectorizer_ct = CountVectorizer(stop_words='english',analyzer='word',input='content', \n",
    "                                 decode_error='ignore', max_df=0.48,min_df=5,\n",
    "                                 token_pattern=r'\\w{1,}', max_features=1625, ngram_range=(1,2)) # to compare two methods, I limit max_features=1625\n",
    "word_ct = vectorizer_ct.fit_transform(processed_post['processed_posts'])\n",
    "word_ct_df = pd.DataFrame(data = word_ct.toarray(), columns = vectorizer_ct.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstract</th>\n",
       "      <th>accept</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>accurate</th>\n",
       "      <th>across</th>\n",
       "      <th>...</th>\n",
       "      <th>year ago</th>\n",
       "      <th>year old</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.12246</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.071834</td>\n",
       "      <td>0.066683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability      able  absolute  absolutely  abstract  accept  according  \\\n",
       "0  0.00000  0.000000  0.000000    0.000000       0.0     0.0        0.0   \n",
       "1  0.00000  0.038307  0.000000    0.000000       0.0     0.0        0.0   \n",
       "2  0.12246  0.044400  0.000000    0.106856       0.0     0.0        0.0   \n",
       "3  0.00000  0.071834  0.066683    0.000000       0.0     0.0        0.0   \n",
       "4  0.00000  0.000000  0.000000    0.000000       0.0     0.0        0.0   \n",
       "\n",
       "   account  accurate  across    ...     year ago  year old       yep  \\\n",
       "0      0.0  0.000000     0.0    ...     0.067997  0.000000  0.083075   \n",
       "1      0.0  0.000000     0.0    ...     0.000000  0.000000  0.000000   \n",
       "2      0.0  0.064077     0.0    ...     0.000000  0.063801  0.000000   \n",
       "3      0.0  0.000000     0.0    ...     0.000000  0.000000  0.000000   \n",
       "4      0.0  0.000000     0.0    ...     0.000000  0.059121  0.000000   \n",
       "\n",
       "        yes  yesterday  yet  young  younger  youtube       yup  \n",
       "0  0.000000        0.0  0.0    0.0      0.0      0.0  0.000000  \n",
       "1  0.000000        0.0  0.0    0.0      0.0      0.0  0.000000  \n",
       "2  0.060355        0.0  0.0    0.0      0.0      0.0  0.081823  \n",
       "3  0.000000        0.0  0.0    0.0      0.0      0.0  0.000000  \n",
       "4  0.055929        0.0  0.0    0.0      0.0      0.0  0.000000  \n",
       "\n",
       "[5 rows x 1625 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: KNN, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = pd.read_csv(\"data/mbti_FE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from random import choice\n",
    "#import lightgbm as lgb\n",
    "#import gc\n",
    "#from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(model, X, target, nsplits=4):\n",
    "    kf = StratifiedShuffleSplit(n_splits=nsplits, random_state=420)\n",
    "    \n",
    "    types = {'EorI':'Extroversion vs. Introversion', 'NorS': 'Intuition vs. Sensing',\n",
    "                 'TorF': 'Thinking vs. Feeling','JorP': 'Judging vs. Perceiving'}\n",
    "    t = time.time()\n",
    "    for col in target.columns:\n",
    "        print(f\"{types[col]}:\")\n",
    "        y = target[col]\n",
    "        all_auc = []\n",
    "        # all_accuracies = []\n",
    "        f_score = []\n",
    "        for train, test in kf.split(X,y):\n",
    "            X_train, X_test, y_train, y_test = X.loc[train], X.loc[test], y[train], y[test]\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            # get the probability of prediction for auc score\n",
    "            preds_act = model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "            # preds = model.predict(X_test)\n",
    "            auc = roc_auc_score(y_test, preds_act)\n",
    "            all_auc.append(auc)\n",
    "                    \n",
    "            fscore = f1_score(preds,y_test)\n",
    "            f_score.append(fscore)\n",
    "            model_name = str(model).split('(')[0]\n",
    "        print(f'Average AUC: {np.mean(all_auc):.3f}; Average fscore: {np.mean(f_score):.3f}')\n",
    "    print(f\"Time use:{time.time()-t:.3f}s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizers and classifiers\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "rf = RandomForestClassifier(random_state=42,min_samples_split=5,\n",
    "                            max_depth=80, criterion='gini',max_features='auto')\n",
    "target = mbti.iloc[:,2:6]\n",
    "# X_tf = np.column_stack((mbti.iloc[:,6:].drop('Sentiment',axis=1),word_tfidf_df))\n",
    "# X_ct = np.column_stack((mbti.iloc[:,6:].drop('Sentiment',axis=1),word_ct_df))\n",
    "\n",
    "X_tf = pd.concat([mbti.iloc[:,6:],word_tfidf_df],axis=1)\n",
    "X_ct = pd.concat([mbti.iloc[:,6:],word_ct_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extroversion vs. Introversion:\n",
      "Average AUC: 0.561; Average fscore: 0.034\n",
      "Intuition vs. Sensing:\n",
      "Average AUC: 0.513; Average fscore: 0.000\n",
      "Thinking vs. Feeling:\n",
      "Average AUC: 0.652; Average fscore: 0.534\n",
      "Judging vs. Perceiving:\n",
      "Average AUC: 0.503; Average fscore: 0.709\n",
      "Time use:144.734s\n"
     ]
    }
   ],
   "source": [
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)\n",
    "model(bagging, X_tf, target, nsplits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extroversion vs. Introversion:\n"
     ]
    }
   ],
   "source": [
    "model(bagging, X_ct, target, nsplits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest on tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extroversion vs. Introversion:\n",
      "Average AUC: 0.567; Average fscore: 0.102\n",
      "Intuition vs. Sensing:\n",
      "Average AUC: 0.564; Average fscore: 0.024\n",
      "Thinking vs. Feeling:\n",
      "Average AUC: 0.716; Average fscore: 0.591\n",
      "Judging vs. Perceiving:\n",
      "Average AUC: 0.552; Average fscore: 0.663\n",
      "Time use:17.391s\n"
     ]
    }
   ],
   "source": [
    "model(rf, X_tf, target, nsplits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest on counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extroversion vs. Introversion:\n",
      "Average AUC: 0.685; Average fscore: 0.000\n",
      "Intuition vs. Sensing:\n",
      "Average AUC: 0.660; Average fscore: 0.000\n",
      "Thinking vs. Feeling:\n",
      "Average AUC: 0.833; Average fscore: 0.701\n",
      "Judging vs. Perceiving:\n",
      "Average AUC: 0.678; Average fscore: 0.758\n",
      "Time use:1104.125s\n"
     ]
    }
   ],
   "source": [
    "#{'n_estimators': 1600, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 80, 'bootstrap': False}\n",
    "model(rf, X_ct, target, nsplits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on tfidf ngram=(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extroversion vs. Introversion:\n",
      "Average AUC: 0.540; Average fscore: 0.222\n",
      "Intuition vs. Sensing:\n",
      "Average AUC: 0.508; Average fscore: 0.097\n",
      "Thinking vs. Feeling:\n",
      "Average AUC: 0.603; Average fscore: 0.555\n",
      "Judging vs. Perceiving:\n",
      "Average AUC: 0.498; Average fscore: 0.629\n",
      "Time use:60.981s\n"
     ]
    }
   ],
   "source": [
    "model(knn, X_tf, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on counter Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extroversion vs. Introversion:\n",
      "Average AUC: 0.538; Average fscore: 0.216\n",
      "Intuition vs. Sensing:\n",
      "Average AUC: 0.530; Average fscore: 0.099\n",
      "Thinking vs. Feeling:\n",
      "Average AUC: 0.601; Average fscore: 0.546\n",
      "Judging vs. Perceiving:\n",
      "Average AUC: 0.504; Average fscore: 0.633\n",
      "Time use:185.176s\n"
     ]
    }
   ],
   "source": [
    "model(knn, X_ct, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rs(model, X, target, nsplits=1):\n",
    "    kf = StratifiedShuffleSplit(nsplits,random_state=420)\n",
    "    \n",
    "    types = {'EorI':'Extroversion vs. Introversion', 'NorS': 'Intuition vs. Sensing',\n",
    "                 'TorF': 'Thinking vs. Feeling','JorP': 'Judging vs. Perceiving'}\n",
    "    t = time.time()\n",
    "    for col in target.columns:\n",
    "        print(f\"{types[col]}:\")\n",
    "        y = target[col]\n",
    "        all_auc = []\n",
    "        # all_accuracies = []\n",
    "        f_score = []\n",
    "        for train, test in kf.split(X,y):\n",
    "            X_train, X_test, y_train, y_test = X.loc[train], X.loc[test], y[train], y[test]\n",
    "            model.fit(X_train, y_train)\n",
    "            print(model.best_params_)\n",
    "            preds = model.predict(X_test)\n",
    "            # get the probability of prediction for auc score\n",
    "            preds_act = model.predict_proba(X_test)[:,1]\n",
    "            # preds = model.predict(X_test)\n",
    "            auc = round(roc_auc_score(y_test, preds_act),3)\n",
    "            f1score = f1_score(preds,y_test)\n",
    "            model_name = str(model).split('(')[0]\n",
    "        \n",
    "        print(f'AUC: {auc:.3f}; f1_score: {f1score:.3f}')\n",
    "    print(f\"Time use:{time.time()-t:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, \n",
    "but rather a fixed number of parameter settings is sampled from the specified distributions. \n",
    "The number of parameter settings that are tried is given by n_iter.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# pprint(random_grid)\n",
    "\n",
    "#{'bootstrap': [True, False],\n",
    "# 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "# 'max_features': ['auto'],\n",
    "# 'min_samples_leaf': [1, 2, 4], default=1 \n",
    "# 'min_samples_split': [2, 5, 10], default=2\n",
    "# 'n_estimators': [100, 200,600, 800, 1000, 1200, 1600, 2000]} default=100, The number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extroversion vs. Introversion:\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1525, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'bootstrap': False}\n",
      "AUC: 0.700; f1_score: 0.000\n",
      "Intuition vs. Sensing:\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 575, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 80, 'bootstrap': False}\n",
      "AUC: 0.685; f1_score: 0.000\n",
      "Thinking vs. Feeling:\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1525, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'bootstrap': False}\n",
      "AUC: 0.838; f1_score: 0.718\n",
      "Judging vs. Perceiving:\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  9.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 40, 'bootstrap': False}\n",
      "AUC: 0.650; f1_score: 0.750\n",
      "Time use:2558.058s\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "# should try n_iter 50?\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, \n",
    "                               verbose=2, random_state=420, n_jobs = -1)\n",
    "'''\n",
    "n_jobs = -1 means using all processors\n",
    "cv = 3 : 3-fold cross validation\n",
    "n_iter = number of parameter settings that are sampled, this trades off runtime\n",
    "verbose = 2 the higher, the more messages\n",
    "'''\n",
    "\n",
    "# Fit the random search model\n",
    "'''\n",
    "More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, \n",
    "but raising each will increase the run time. \n",
    "Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.\n",
    "'''\n",
    "model_rs(rf_random, X_ct, target, nsplits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RF and KNN models **DONE**\n",
    "- Visualization to compare the model performances\n",
    "- How to explain EorI things to the auidence in an easy way\n",
    "- PCA "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
